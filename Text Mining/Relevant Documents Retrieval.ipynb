{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving Relevant Documents using TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import PyPDF2 as pypdf2\n",
    "import collections\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from os import listdir\n",
    "import os.path\n",
    "from os.path import isfile, join\n",
    "import docx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Path of the folder containing documents\n",
    "- Provide the folder path of your interest\n",
    "- Reading the filenames that end with .pdf or .docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'listdir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d2605da558f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#reading .pdf and .docx filenames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfiles_name\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfile\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.pdf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;32mor\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.docx'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'listdir' is not defined"
     ]
    }
   ],
   "source": [
    "file_path=\"insert a path containing .pdf and .docx files\"\n",
    "\n",
    "#reading .pdf and .docx filenames\n",
    "files_name= [file for file in listdir(file_path) if isfile(join(file_path,file))and (file.endswith('.pdf')or file.endswith('.docx') )]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Reading the files and storing the text as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docx_reader(file):\n",
    "\n",
    "    doc = docx.Document(file)\n",
    "    text = ''\n",
    "    for para in doc.paragraphs:\n",
    "        text += para.text\n",
    "\n",
    "    return(text)\n",
    "\n",
    "\n",
    "def pdf_reader(file):\n",
    "    pdf_object = open(file, 'rb')\n",
    "    pdfreader = pypdf2.PdfFileReader(pdf_object)\n",
    "\n",
    "    num_pages = pdfreader.numPages\n",
    "    c = 0\n",
    "    text = \"\"\n",
    "    while c < num_pages:\n",
    "        pageObj = pdfreader.getPage(c)\n",
    "        c += 1\n",
    "        text += pageObj.extractText()\n",
    "\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Cleaning the text and Calculating the frequency of words\n",
    "- Split the text into words \n",
    "- Convert into lower\n",
    "- Remove punctuations\n",
    "- Remove stop words\n",
    "- Calculate frequency of words in each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenisation_df(file_content):\n",
    "    #converting into lower\n",
    "    d = [x.lower() for x in file_content.split()]\n",
    "    file_content = ' '.join(d)\n",
    "    #removing punctuations\n",
    "    table = str.maketrans({key: ' ' for key in string.punctuation})\n",
    "    file_content = file_content.translate(table)\n",
    "    # removing stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    s = [x for x in file_content.split() if x not in stop_words]\n",
    "    file_content = ' '.join(s)\n",
    "    # calcuating frequency of words\n",
    "    tokens = word_tokenize(file_content)\n",
    "    d = collections.Counter(tokens)\n",
    "    # Returning a data frame with File name,words,frequency\n",
    "    keywords_df = pd.DataFrame.from_dict(d, orient='index').reset_index()\n",
    "    keywords_df = keywords_df.rename(columns={'index': 'event', 0: 'freq'})\n",
    "    return (keywords_df)\n",
    "\n",
    "#helper function to read .pdf or .docx file\n",
    "\n",
    "def text(x,z):\n",
    "    \n",
    "    switcher = {\n",
    "        '.pdf': pdf_reader,\n",
    "        '.docx': docx_reader,\n",
    "\n",
    "    }\n",
    "\n",
    "    y = switcher.get(x)\n",
    "\n",
    "    return (y(z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Calculating the TF-IDF score for each word in every document\n",
    "- Calculate Term Frequency(TF) of each word in a document\n",
    "- Calculae Inverse Document Frequency (IDF) of each word in a document\n",
    "- Calculate TF-IDF score i.e TF * IDF of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_df(files_name):\n",
    "    \n",
    "    \n",
    "    keywords_final = pd.DataFrame()\n",
    "#TF\n",
    "    for file in files_name:\n",
    "        \n",
    "        #reading the text of the file\n",
    "        filename=join(file_path,file)\n",
    "\n",
    "        tex=text(os.path.splitext(filename)[1],filename)\n",
    "        \n",
    "        #Cleaning the text and calculating the frequency of words\n",
    "        keywords_frequency=tokenisation_df(tex)\n",
    "\n",
    "        keywords_frequency['File']=file\n",
    "        \n",
    "        # Calculating TF of each word\n",
    "        keywords_frequency['TF']=keywords_frequency['freq']/sum(keywords_frequency['freq'])\n",
    "\n",
    "\n",
    "        keywords_final=keywords_final.append(keywords_frequency, ignore_index=True)\n",
    "\n",
    "\n",
    "    keywords_final['idf']=0\n",
    "\n",
    "#IDF\n",
    "    \n",
    "    for i in range(len(keywords_final)):\n",
    "\n",
    "        sub=keywords_final.loc[keywords_final['event']==keywords_final['event'][i]]\n",
    "\n",
    "        # Caculating IDF of each word\n",
    "        idf =np.log(len(np.unique(keywords_final['File']))/(1+len(sub)))\n",
    "\n",
    "        if idf<0:\n",
    "            keywords_final.loc[i, 'idf']=1\n",
    "        else:\n",
    "            keywords_final.loc[i, 'idf']=idf\n",
    "            \n",
    "    \n",
    "    # Calculating TF-IDF score for each word\n",
    "    keywords_final['Tf_idf']=keywords_final['TF']*keywords_final['idf']\n",
    "    return keywords_final\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Ranking the files based on user search\n",
    "- Cleaning the input search to identify key words\n",
    "- Based on the keywords each file is assigned a final score\n",
    "- Score is calculated as the sum of TF-IDF scores of keywords with respect to each file\n",
    "- Then the files are sorted based on score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rank_df(input_content):\n",
    "    \n",
    "    rank_matrix = pd.DataFrame()\n",
    "\n",
    "    #user search\n",
    "    input_text=input_content\n",
    "    \n",
    "    #cleaning the user inputs\n",
    "    token_df=tokenisation_df(input_text)\n",
    "    keywords_final=main_df(files_name)\n",
    "    #keywords_final=df\n",
    "\n",
    "    for i in files_name:\n",
    "\n",
    "        score=0\n",
    "\n",
    "        for w in token_df['event']:\n",
    "            \n",
    "            #calculating the score for each file\n",
    "            sub_df=keywords_final.loc[(keywords_final['event']==w) & (keywords_final['File']==i)]\n",
    "\n",
    "            if len(sub_df)==0:\n",
    "                score=score\n",
    "            else:\n",
    "\n",
    "                score=score+float(sub_df['Tf_idf'])\n",
    "\n",
    "        rank_matrix=rank_matrix.append({'File':i,'Score':score},ignore_index=True)\n",
    "\n",
    "    return(rank_matrix)\n",
    "\n",
    "#input here\n",
    "Relevancy_files= rank_df('enter keywords to search')\n",
    "\n",
    "#Final list of files sorted based on rank\n",
    "Relevancy_files= Relevant_files.sort_values(by='Score', ascending=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
